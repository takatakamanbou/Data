{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex14notebookA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRSQ9BhnoKMt76e33i3c/p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/Data/blob/main/ex14notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# Data2021 ex14notebookA\n",
        "\n",
        "<img width=64 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/Data-logo14.png\"> https://www-tlab.math.ryukoku.ac.jp/wiki/?Data/2021\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4-TmUdCd9FJ"
      },
      "source": [
        "## 準備\n",
        "\n",
        "Google Colab の Notebook では， Python というプログラミング言語のコードを動かして計算したりグラフを描いたりもできます．\n",
        "Python は，データ分析や機械学習・人工知能分野ではメジャーなプログラミング言語ですが，それを学ぶことはこの授業の守備範囲ではありません．ですので，以下の所々に現れるプログラムっぽい記述の内容は，理解できなくて構いません．\n",
        "\n",
        "以下，コードセルを上から順に実行していってね．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ8M3640S1W7"
      },
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "# Python 用の機械学習ライブラリ scikit-learn の線形回帰モデルおよびロジスティック回帰モデル\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# ゴリゴリくん\n",
        "dfGori = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/ex08gorigori.csv', header=0)\n",
        "\n",
        "# 模試入試\n",
        "dfExam = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/ex14exam.csv', header=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H45eNIcd2xB"
      },
      "source": [
        "----\n",
        "## 「データ分析」から「多変量解析及び演習」・「機械学習I/II」へ\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM4RlkyhP5yo"
      },
      "source": [
        "----\n",
        "### 多変量解析？\n",
        "\n",
        "「データ分析」で扱ったデータは，変数の数が1つまたは2つに限られていました．\n",
        "「多変量解析及び演習」では，変数（説明変数）が複数あるようなデータ（多変量データ）を扱うデータ分析・統計手法である **多変量解析** について学びます．\n",
        "多変量解析ができるようになれば，より多様なデータ／より複雑な問題に取り組むことができるようになります．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSfmT8wOA7Ln"
      },
      "source": [
        "「多変量解析」の手法には，例えば次のようなものがあります．\n",
        "\n",
        "- **重回帰分析**\n",
        "- **主成分分析**\n",
        "- **クラスター分析**／**クラスタリング**\n",
        "\n",
        "以下では，重回帰分析について少し説明します．\n",
        "それ以外のものはここでは説明しませんので，興味があれば書籍を探したりネットで調べたりしてみるとよいでしょう．\n",
        "数理情報学科3年次科目「パターン情報処理」（2019年度以前入学生向け）の講義資料にも関連の説明があります\n",
        "\n",
        "2021年度「パターン情報処理」のページ: https://www-tlab.math.ryukoku.ac.jp/wiki/?PIP/2021 （第12回から第15回）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlfNl6Y-Uxuv"
      },
      "source": [
        "----\n",
        "### 重回帰分析？\n",
        "\n",
        "**重回帰分析** は，回帰分析を説明変数が2つ以上ある場合に拡張したものです（対比させるときは，説明変数1つの回帰分析を **単回帰**（分析） と呼ぶことがあります）．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiCkLUmVcLBJ"
      },
      "source": [
        "#### 「データ分析」で学んだ回帰分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCHvtTlCUwNe"
      },
      "source": [
        "\n",
        "「データ分析」の授業で扱った回帰分析では，与えられるデータは\n",
        "\n",
        "$$\n",
        "(x_n, y_n)\\quad (n = 1, 2, \\dots , N)\n",
        "$$\n",
        "\n",
        "という形で，説明変数（$x$）も被説明変数（$y$）も一つだけでした．回帰分析の目的は，このようなデータにうまくあてはまる直線\n",
        "\n",
        "$$y = ax+b$$\n",
        "\n",
        "を見つける（パラメータ $a, b$ を決める），というものでした．この授業では，気温を説明変数，アイス売上数を被説明変数として，気温からアイス売上数を予測する，といった問題などを例として考えました．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dexkIJ91LbZ6"
      },
      "source": [
        "# ゴリゴリくん\n",
        "dfGori.head()  # 最初の5件だけ表示"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_lPOJfR8W2M"
      },
      "source": [
        "# データ\n",
        "X = dfGori['気温'].values  # 説明変数\n",
        "Y = dfGori['アイス売上数'].values  # 被説明変数\n",
        "\n",
        "# 最小二乗法による直線のあてはめ\n",
        "lr = LinearRegression().fit(X.reshape(-1, 1), Y)\n",
        "a, b = lr.coef_[0], lr.intercept_\n",
        "\n",
        "# 予測\n",
        "Yest = a * X + b\n",
        "\n",
        "# グラフを描く\n",
        "Xr = np.array([0, 40])\n",
        "fig, ax = plt.subplots(1, facecolor='white', figsize=(9, 6))\n",
        "ax.set_xlabel('Temparature')\n",
        "ax.set_ylabel('number of GoriGori')\n",
        "ax.scatter(X, Y)\n",
        "ax.plot(Xr, a*Xr + b, color='red')\n",
        "ax.set_xlim(0, 40)\n",
        "ax.set_ylim(-5, 125)\n",
        "plt.show()\n",
        "\n",
        "# 得られたパラメータを表示\n",
        "print(f'a = {a:.3f}, b = {b:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoSjBJEKcQ-v"
      },
      "source": [
        "#### 重回帰分析の問題の例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nohzdgXTAYIi"
      },
      "source": [
        "では，説明変数が2つ以上ある場合とはどういうものか，実際のデータの例をあげてみます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akufH4SAF55I"
      },
      "source": [
        "# Python 用の機械学習ライブラリ scikit-learn からデータを読み込む\n",
        "#    Boston Housing データ（アメリカ・ボストンの住宅の部屋数，価格や地域の犯罪率等）\n",
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "\n",
        "boston = load_boston()\n",
        "dfBoston = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "\n",
        "# 本当は説明変数は 13 あるが，ここではそのうち\n",
        "# CRIM（犯罪発生率），RM（平均部屋数），RAD（高速道路へのアクセスのしやすさ）\n",
        "# のみを使う\n",
        "dfBoston.drop(columns=['ZN', 'INDUS', 'CHAS', 'NOX', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT'], inplace=True)\n",
        "dfBoston['MEDV'] = boston.target # 被説明変数（住宅価格）\n",
        "\n",
        "dfBoston"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boVZLjGLGG0D"
      },
      "source": [
        "上記は，「Boston Housing データセット」としてよく知られたものです．\n",
        "アメリカ・ボストンの町ごとの住宅の部屋数，価格や地域の犯罪率等のデータです（後述の「参考」のリンク参照）．\n",
        "実際のデータはもっと変数の数が多いのですが，わかりやすさを優先して，ここでは次の4つの変数のみを取り出しています．サンプルサイズは 506 です．\n",
        "\n",
        "- CRIM: 犯罪の率（$x_1$ と表すことにします）\n",
        "- RM:  平均の部屋数（$x_2$）\n",
        "- RAD: 高速道路へのアクセスの良さ（$x_3$）\n",
        "- MEDV: その町の住宅価格の中央値（$y$）\n",
        "\n",
        "\n",
        "\n",
        "参考:\n",
        "- http://lib.stat.cmu.edu/datasets/boston\n",
        "- https://www.atmarkit.co.jp/ait/articles/2006/24/news033.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zII3SHHlAOgc"
      },
      "source": [
        "このようなデータが与えられた場合，CRIM, RM, RAD の値から MEDV の値を求める式を例えば\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2 + w_3x_3\n",
        "$$\n",
        "\n",
        "という形として，パラメータ $w_0, w_1, w_2, w_3$ をうまく決めることができれば，ボストンのいろんな町の住宅価格を予測できるようになると期待できます．\n",
        "重回帰分析は，このようなデータ分析を行うための手法です．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8mAMDUpc1NO"
      },
      "source": [
        "#### 実際に重回帰分析してみよう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGb4h3Y7V4qG"
      },
      "source": [
        "「Boston Housing データセット」（の一部を取り出したもの）の重回帰分析を実際にやってみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCTt4Igyauqw"
      },
      "source": [
        "# データ\n",
        "X = dfBoston.drop(columns='MEDV').values # 説明変数\n",
        "Y = dfBoston['MEDV'].values  # 被説明変数\n",
        "\n",
        "# 最小二乗法による平面のあてはめ\n",
        "lr = LinearRegression().fit(X, Y)\n",
        "w0 = lr.intercept_\n",
        "w = lr.coef_\n",
        "\n",
        "# 得られたパラメータを表示\n",
        "print(f'w0 = {w0:.1f},   w1 = {w[0]:.3f},   w2 = {w[1]:.2f},   w3 = {w[2]:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U0007mbYBIm"
      },
      "source": [
        "上記の結果は，3つの説明変数の値から住宅価格を予測する式が\n",
        "$$\n",
        "y = -27.1 - 0.165x_1 + 8.24x_2 - 0.161x_3\n",
        "$$\n",
        "と求まることを意味しています．これを使って実際にデータから住宅価格を予測してみると..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzsWYgv5Y4Ei"
      },
      "source": [
        "# 予測値の計算\n",
        "Y_est = lr.predict(X)\n",
        "for n in range(10):  # 最初の10件分\n",
        "    print(f'{n} 説明変数の値: {X[n]}  被説明変数の予測値: {Y_est[n]:.2f}    真の値: {Y[n]:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmPZtp8eZgIF"
      },
      "source": [
        "これだけでは予測がうまく言ってるかどうか判断が難しい（注）ですが，それっぽい値を出しているようです\n",
        "\n",
        "<font size=\"-1\">注） 単回帰の場合に **決定係数** を考えたように，重回帰分析でも当てはまりの良さや結果の信頼性を評価する方法があります．</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbCbmJFPaZVc"
      },
      "source": [
        "以下に，本来のデータを全て使った場合（$D=13$）の分析をするコードも載せておきます．実行してみてください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFPzLL5lV78W"
      },
      "source": [
        "# データ\n",
        "X = boston.data # 説明変数\n",
        "Y = boston.target # 被説明変数\n",
        "\n",
        "# 最小二乗法による平面のあてはめ\n",
        "lr = LinearRegression().fit(X, Y)\n",
        "w0 = lr.intercept_\n",
        "w = lr.coef_\n",
        "\n",
        "# 得られたパラメータを表示\n",
        "print(w0, w)\n",
        "\n",
        "# 予測値の計算\n",
        "Y_est = lr.predict(X)\n",
        "for n in range(10):  # 最初の10件分\n",
        "    print(f'{n}  被説明変数の予測値: {Y_est[n]:.2f}    真の値: {Y[n]:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Zorls_CB3V"
      },
      "source": [
        "#### 重回帰分析の定式化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4cpjAglJvkF"
      },
      "source": [
        "説明変数の数を $D$，データの数（サンプルサイズ）を $N$とすると，重回帰分析で扱うデータは次のように表せます．\n",
        "\n",
        "$$\n",
        "(x_{n, 1}, x_{n, 2}, \\dots, x_{n, D}, y_n)\\quad (n = 1, 2, \\dots , N)\n",
        "$$\n",
        "\n",
        "$n$ はデータの番号，$x_{n, 1}$ から $x_{n, D}$ までが $n$ 番目のデータの説明変数の値，$y_n$ が被説明変数の値を表わします． 上記の例では $D=3$ および $N=506$ でした．\n",
        "\n",
        "このデータに当てはめる式は，次のようなものです．\n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2 + \\dots + w_Dx_D\n",
        "$$\n",
        "単回帰の式 $y = ax+b$ が2次元平面上の直線の式だったのに対して，この式は，データが広がる $(D+1)$ 次元空間中の平面（$D$次元超平面）を表わします．\n",
        "平面の位置や傾きを決めるパラメータは $w_0$ から $w_D$ までの $(D+1)$ 個あります．\n",
        "これらのパラメータは，単回帰の場合と同様に，最小二乗法によって求めることができます（注）．\n",
        "\n",
        "<font size=\"-1\">注） 単回帰の場合よりも複雑にはなりますが，微積分と線形代数を駆使してパラメータの連立方程式である「正規方程式」を導出することができます．\n",
        "その辺の話はこの授業ではしません．「多変量解析及び演習」を待つか，上述の「パターン情報処理」第13回資料などをどうぞ．\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSI2_lgJc9GH"
      },
      "source": [
        "##### チョットナニイッテルカワカラナイデスネー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWO_6hgOOQdG"
      },
      "source": [
        "$D$個の説明変数と1つの被説明変数から成るデータが与えられた場合，これらのデータは$(D+1)$ 次元空間に散らばります．\n",
        "重回帰分析は，この空間の中で，データたちに最もよく当てはまる $D$次元平面を求めているわけですが，$(D+1)$空間とか図に描けない＆想像できないから，チョットナニイッテルカワカラナイデスネー\n",
        "\n",
        "というわけで， $D=2$ の場合について，乱数で作ったデータの散布図とそれに平面を当てはめたものを3次元空間に描いてみます．\n",
        "\n",
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/ex14-regression.png\" width=\"480\">\n",
        "\n",
        "青い点が個々のデータです．\n",
        "赤いのは，これらのデータに当てはめた平面を表わします．\n",
        "この平面の式は\n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2\n",
        "$$\n",
        "という形です．パラメータ $(w_0, w_1, w_2)$ の値は，この赤い平面が青いデータ点にもっともよくあてはまるように求められます．\n",
        "\n",
        "重回帰分析とは，このような平面あてはめをより高い次元の空間に一般化したものとなっています．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pEs3oIHhNSL"
      },
      "source": [
        "上記の例のグラフを動かしていろんな方向から観察してみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCVTEAd-hbfo"
      },
      "source": [
        "N = 50  # サンプルサイズ\n",
        "D = 2   # データの次元数\n",
        "\n",
        "# 乱数を使ってデータを生成\n",
        "Xtrain = np.random.rand(N, D)\n",
        "Ytrue = 1.0 + Xtrain @ [2.0, 3.0]\n",
        "Ytrain = Ytrue + 0.2*np.random.randn(N)  # y = 2x_1 + 3x_2 + 1\n",
        "\n",
        "# 平面当てはめ\n",
        "lr = LinearRegression().fit(Xtrain, Ytrain)\n",
        "w0 = lr.intercept_\n",
        "w1, w2 = lr.coef_\n",
        "\n",
        "# 得られたパラメータを表示\n",
        "print(f'w0 = {w0:.4f}, w1 = {w1:.3f}, w2 = {w2:.3f}')\n",
        "\n",
        "# 予測値の計算とグラフ描画の準備\n",
        "x = np.arange(0, 1, 0.1)\n",
        "y = np.arange(0, 1, 0.1)\n",
        "xx, yy = np.meshgrid(x, y)\n",
        "zz = w0 + w1 * xx + w2 * yy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oGqZmG7jD_Y"
      },
      "source": [
        "データの散布図と，当てはめた平面を可視化してみよう．\n",
        "\n",
        "- elevation を変えると上下に視点が移動します\n",
        "- azimuth を変えると左右に視点が移動します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk0ufdq_hkLa"
      },
      "source": [
        "#@title 平面当てはめ {run: \"auto\"}\n",
        "fig = plt.figure(facecolor='white', figsize=(10, 7.5))\n",
        "ax = fig.add_subplot(111, projection='3d', xlabel='x_2', ylabel='x_1', zlabel='y')\n",
        "ax.scatter(Xtrain[:, 0], Xtrain[:, 1], Ytrain)\n",
        "ax.plot_wireframe(xx, yy, zz, color='red')\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_ylim(0, 1)\n",
        "elevation = 50 #@param {type:\"slider\", min:0, max:60, step:10} \n",
        "azimuth = 20 #@param {type:\"slider\", min:0, max:90, step:10}\n",
        "ax.view_init(elevation, azimuth)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywUB7T94fUbs"
      },
      "source": [
        "----\n",
        "### 重回帰分析のさらに先へ\n",
        "\n",
        "重回帰分析のさらに先にも，いろいろ面白いものがあります．\n",
        "「機械学習」で学ぶことになるだろう内容ですが，一部簡単に紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBzuUWZwgY15"
      },
      "source": [
        "#### 多項式の当てはめ and more\n",
        "\n",
        "重回帰分析の式をちょっと変形すると，単回帰で扱ったデータ\n",
        "$$\n",
        "(x_n, y_n)\\quad (n = 1, 2, \\dots , N)\n",
        "$$\n",
        "に対して，直線ではなく $D$ 次多項式\n",
        "\n",
        "$$ y = w_0 + w_1x + w_2x^2 + \\dots +\n",
        "w_Dx^D\n",
        "$$\n",
        "\n",
        "を当てはめることができます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOnZix3kENv"
      },
      "source": [
        "「ゴリゴリくん」でやってみましょう．以下のスライダーを動かして多項式の次数 $D$ を 1 から大きくしていってみよう．\n",
        "$D$ を大きくしていくと，大変なことになります...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqUbefwMkOnR"
      },
      "source": [
        "#@title あてはめる多項式の次数 {run: \"auto\"} \n",
        "D = 1 #@param {type:\"slider\", min:1, max:20}\n",
        "\n",
        "# ゴリゴリくん\n",
        "X, Y = dfGori['気温'], dfGori['アイス売上数']\n",
        "N = Y.shape[0]\n",
        "\n",
        "# データの行列 XX をつくる\n",
        "XX = np.empty((N, D+1))\n",
        "XX[:, 0] = 1\n",
        "for d in range(1, D+1):\n",
        "    XX[:, d] = X**d\n",
        "\n",
        "# 最小二乗法で多項式を求める\n",
        "lr = LinearRegression().fit(XX, Y)\n",
        "f_est = np.poly1d(np.hstack((lr.intercept_, lr.coef_[1:]))[::-1])\n",
        "\n",
        "# グラフを描く\n",
        "xr = np.linspace(0, 40, 100)\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(8, 6))\n",
        "ax.set_xlabel('Temparature')\n",
        "ax.set_ylabel('number of GoriGori')\n",
        "ax.scatter(X, Y, color=\"red\")\n",
        "ax.plot(xr, f_est(xr), '-', label=f\"D = {D}\")\n",
        "ax.set_xlim(0, 40)\n",
        "ax.set_ylim(-5, 125)\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# 得られた多項式を表示\n",
        "print(f_est)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuFToOnhomd-"
      },
      "source": [
        "この問題のように，与えられたデータに複雑な関数を当てはめる問題（**関数近似** と言うことがあります）は，科学技術の様々な問題に現れる重要なものです．\n",
        "関数近似問題への取り組み方（アプローチ）にも様々なものがありますので，数理・情報科学課程のあちこちの授業で出会うことになるかもしれません（微積分で学んだ「テイラー展開」も，その特殊な例と言えます）．\n",
        "いわゆる人工知能の基礎となる話でもありますので，「機械学習I/II」でも取り上げます．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig3dnpNMWPdh"
      },
      "source": [
        "#### ロジスティック回帰\n",
        "\n",
        "これまでの回帰分析の話では，被説明変数はすべて量的なものでした（注）．\n",
        "**ロジスティック回帰** は，被説明変数が質的なものでかつ2種類の値しかとらないようなデータを分析するための手法です．\n",
        "\n",
        "<font size=\"-1\">注）  説明変数の方については，質的な変数が混じっていたとしても，ダミー変数に変換してやることで，量的な変数とみなして普通に回帰分析することができます．</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qknwxXBvX3MM"
      },
      "source": [
        "例えばこんなん...．ある模擬試験を受験したのちある大学の入試を受験したひと300人分の，模試の2科目の点数と，大学入試に合格したか否か（1なら合格），のデータです．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfJozm3iXqnE"
      },
      "source": [
        "# 模試入試データ\n",
        "dfExam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzbQHZr5X_ij"
      },
      "source": [
        "模試の点数から合否を予測する式を作ることができれば，このデータに含まれない受験生の合否を予測できるかもしれません．このような問題を扱うのが，ロジスティック回帰です．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sndzEd4UZKHO"
      },
      "source": [
        "とりあえず，説明変数（模試2科目の点数）の散布図を描いてみるとこんなんなります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHPJdE7LZUs8"
      },
      "source": [
        "# データ\n",
        "X = dfExam[['模試数学', '模試英語']].values # 説明変数\n",
        "Y = dfExam['入試合否'].values # 被説明変数\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, facecolor='white', figsize=(12, 6))\n",
        "# 普通に散布図\n",
        "ax[0].scatter(X[:, 0], X[:, 1])\n",
        "ax[0].set_xlim(0, 100)\n",
        "ax[0].set_ylim(0, 100)\n",
        "ax[0].set_xlabel('Math')\n",
        "ax[0].set_ylabel('English')\n",
        "# 入試合否で色分け\n",
        "X0 = X[Y == 0, :]\n",
        "X1 = X[Y == 1, :]\n",
        "ax[1].scatter(X0[:, 0], X0[:, 1])\n",
        "ax[1].scatter(X1[:, 0], X1[:, 1])\n",
        "ax[1].set_xlim(0, 100)\n",
        "ax[1].set_ylim(0, 100)\n",
        "ax[1].set_xlabel('Math')\n",
        "ax[1].set_ylabel('English')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mA-yDMuashv"
      },
      "source": [
        "右の方は，合格したひとをオレンジで，不合格だったひとを青で表しています．さらに，入試合否も使って3次元の散布図を描くと，こんなことになります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aakd_kgcbQV2"
      },
      "source": [
        "# 模試の点数と入試合否の3次元の散布図\n",
        "fig = plt.figure(facecolor='white', figsize=(10, 7.5))\n",
        "ax = fig.add_subplot(111, projection='3d', xlabel='Math', ylabel='English', zlabel='result')\n",
        "X0 = X[Y == 0, :]\n",
        "X1 = X[Y == 1, :]\n",
        "ax.scatter(X0[:, 0], X0[:, 1], Y[Y == 0])\n",
        "ax.scatter(X1[:, 0], X1[:, 1], Y[Y == 1])\n",
        "ax.set_xlim(0, 100)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_zlim(0, 1)\n",
        "elevation = 70\n",
        "azimuth = 240\n",
        "ax.view_init(elevation, azimuth)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS37lBZufMva"
      },
      "source": [
        "以下，ロジスティック回帰の式などの説明は省略して，結果を図示するだけにします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2adi5xUXfh2z"
      },
      "source": [
        "# データ\n",
        "X = dfExam[['模試数学', '模試英語']].values # 説明変数\n",
        "Y = dfExam['入試合否'].values # 被説明変数\n",
        "\n",
        "# ロジスティック回帰\n",
        "lr = LogisticRegression(C=1e6).fit(X, Y)\n",
        "w0 = lr.intercept_[0]\n",
        "w = lr.coef_[0]\n",
        "\n",
        "# パラメータを表示\n",
        "print(w0, w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk2VE1urggsj"
      },
      "source": [
        "# 予測値\n",
        "x0 = np.arange(0, 100, 5)\n",
        "x1 = np.arange(0, 100, 5)\n",
        "xx0, xx1 = np.meshgrid(x0, x1)\n",
        "XX = np.vstack((xx0.reshape(-1), xx1.reshape(-1))).T\n",
        "yy = lr.predict_proba(XX)[:, 1].reshape(xx0.shape)\n",
        "\n",
        "# 可視化\n",
        "fig = plt.figure(facecolor='white', figsize=(10, 7.5))\n",
        "ax = fig.add_subplot(111, projection='3d', xlabel='Math', ylabel='English', zlabel='result')\n",
        "X0 = X[Y == 0, :]\n",
        "X1 = X[Y == 1, :]\n",
        "ax.scatter(X0[:, 0], X0[:, 1], Y[Y == 0])\n",
        "ax.scatter(X1[:, 0], X1[:, 1], Y[Y == 1])\n",
        "ax.plot_wireframe(xx0, xx1, yy, color='green')\n",
        "ax.set_xlim(0, 100)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_zlim(0, 1)\n",
        "elevation = 70\n",
        "azimuth = 240\n",
        "ax.view_init(elevation, azimuth)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ausDmjZBjcMG"
      },
      "source": [
        "上記の緑色は，ロジスティック回帰によって得られた予測式が表す曲面です．この式によって得られる予測値は，「合格する確率」と解釈することができます．\n",
        "\n",
        "以下は，予測値が 0.5 に等しくなるような説明変数の値の集合（緑色の直線）を描いたものです．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ4yagRwgquO"
      },
      "source": [
        "fig, ax = plt.subplots(1, facecolor='white', figsize=(6, 6))\n",
        "X0 = X[Y == 0, :]\n",
        "X1 = X[Y == 1, :]\n",
        "ax.scatter(X0[:, 0], X0[:, 1])\n",
        "ax.scatter(X1[:, 0], X1[:, 1])\n",
        "xr = np.linspace(0, 100)\n",
        "ax.plot(xr, -(w[0]*xr + w0)/w[1], color='green')\n",
        "ax.set_xlim(0, 100)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_xlabel('Math')\n",
        "ax.set_ylabel('English')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Ue4Vz-ksle"
      },
      "source": [
        "大雑把な考察ですが，この直線の傾きから，模試英語の点数よりも模試数学の点数の大小の方が入試合否に強く影響していることがうかがえます．\n",
        "\n",
        "ここでは説明しませんが，このロジスティック回帰を発展させて，ずっとずっと複雑な仕組みを考えると，文字認識，音声認識，画像認識といった問題も扱えるようになります．\n",
        "そういう話は，「機械学習I/II」の授業に出てくるでしょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA8twjzrrQOm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzASFL_9DM4J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
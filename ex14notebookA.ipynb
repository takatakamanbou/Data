{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOmXPf+OCOU4yIEuRxAyqK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/Data/blob/2023/ex14notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# Data2023 ex14notebookA\n",
        "\n",
        "<img width=64 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/Data-logo14.png\"> https://www-tlab.math.ryukoku.ac.jp/wiki/?Data/2023\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4-TmUdCd9FJ"
      },
      "source": [
        "## 準備\n",
        "\n",
        "以下，コードセルを上から順に実行していってね．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ8M3640S1W7"
      },
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "# Python 用の機械学習ライブラリ scikit-learn の線形回帰モデルおよびロジスティック回帰モデル\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# ゴリゴリくん\n",
        "dfGori = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/ex08gorigori.csv', header=0)\n",
        "\n",
        "# 模試入試\n",
        "dfExam = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/Data/ex14exam.csv', header=0)\n",
        "\n",
        "# 小テスト + 睡眠時間 vs 期末テスト\n",
        "dfScore = pd.read_csv('https://github.com/ghmagazine/python_stat_sample/raw/master/data/ch12_scores_reg.csv')\n",
        "dfScore.drop(columns='通学方法', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H45eNIcd2xB"
      },
      "source": [
        "----\n",
        "## 「データ分析」から「多変量解析及び演習」・「機械学習I/II」へ\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM4RlkyhP5yo"
      },
      "source": [
        "----\n",
        "### 多変量解析？\n",
        "\n",
        "「データ分析」で扱ったデータは，変数の数が1つまたは2つに限られていました．\n",
        "「多変量解析及び演習」では，変数（説明変数）が複数あるようなデータ（多変量データ）を扱うデータ分析・統計手法である **多変量解析** について学びます．\n",
        "多変量解析ができるようになれば，より多様なデータ／より複雑な問題に取り組むことができるようになります．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSfmT8wOA7Ln"
      },
      "source": [
        "「多変量解析」の手法には，例えば次のようなものがあります．\n",
        "\n",
        "- **重回帰分析**\n",
        "- **主成分分析**\n",
        "- **クラスター分析**／**クラスタリング**\n",
        "\n",
        "以下では，重回帰分析について少し説明します．\n",
        "それ以外のものはここでは説明しませんので，興味があれば書籍を探したりネットで調べたりしてみるとよいでしょう．また，以下のリンク先に，2023年度「多変量解析及び演習」のページがあります．講義資料は Colab Notebook として自由に閲覧できますので，よかったらどうぞ．\n",
        "https://www-tlab.math.ryukoku.ac.jp/wiki/?MVA/2023\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlfNl6Y-Uxuv"
      },
      "source": [
        "----\n",
        "### 重回帰分析？\n",
        "\n",
        "**重回帰分析** は，回帰分析を説明変数が2つ以上ある場合に拡張したものです（対比させるときは，説明変数1つの回帰分析を **単回帰**（分析） と呼ぶことがあります）．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiCkLUmVcLBJ"
      },
      "source": [
        "#### 「データ分析」で学んだ回帰分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCHvtTlCUwNe"
      },
      "source": [
        "\n",
        "「データ分析」の授業で扱った回帰分析では，与えられるデータは\n",
        "\n",
        "$$\n",
        "(x_n, y_n)\\quad (n = 1, 2, \\dots , N)\n",
        "$$\n",
        "\n",
        "という形で，説明変数（$x$）も被説明変数（$y$）も一つだけでした．回帰分析の目的は，このようなデータにうまくあてはまる直線\n",
        "\n",
        "$$y = ax+b$$\n",
        "\n",
        "を見つける（パラメータ $a, b$ を決める），というものでした．この授業では，気温を説明変数，アイス売上数を被説明変数として，気温からアイス売上数を予測する，といった問題などを例として考えました．\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dexkIJ91LbZ6"
      },
      "source": [
        "# ゴリゴリくん\n",
        "dfGori.head()  # 最初の5件だけ表示"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_lPOJfR8W2M"
      },
      "source": [
        "# データ\n",
        "X = dfGori['気温'].values  # 説明変数\n",
        "Y = dfGori['アイス売上数'].values  # 被説明変数\n",
        "\n",
        "# 最小二乗法による直線のあてはめ\n",
        "lr = LinearRegression().fit(X.reshape(-1, 1), Y)\n",
        "a, b = lr.coef_[0], lr.intercept_\n",
        "\n",
        "# 予測\n",
        "Yest = a * X + b\n",
        "\n",
        "# グラフを描く\n",
        "Xr = np.array([0, 40])\n",
        "fig, ax = plt.subplots(1, facecolor='white', figsize=(9, 6))\n",
        "ax.set_xlabel('Temparature')\n",
        "ax.set_ylabel('number of GoriGori')\n",
        "ax.scatter(X, Y)\n",
        "ax.plot(Xr, a*Xr + b, color='red')\n",
        "ax.set_xlim(0, 40)\n",
        "ax.set_ylim(-5, 125)\n",
        "plt.show()\n",
        "\n",
        "# 得られたパラメータを表示\n",
        "print(f'a = {a:.3f}, b = {b:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoSjBJEKcQ-v"
      },
      "source": [
        "#### 重回帰分析の問題の例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nohzdgXTAYIi"
      },
      "source": [
        "では，説明変数が2つ以上ある場合とはどういうものか，実際のデータの例をあげてみます．"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfScore"
      ],
      "metadata": {
        "id": "3XT4wlOUMB3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで扱うデータは，\n",
        "\n",
        "「Pythonで理解する統計解析の基礎」 谷合廣紀，辻 真吾，技術評論社，2018.\n",
        "\n",
        "に掲載されているものです（以下の GitHub サイトで公開されています． https://github.com/ghmagazine/python_stat_sample ）．\n",
        "ある大学のある授業を受講している20人について，小テストの点数，期末テストの点数，睡眠時間，3つの情報を集めたものとなっています（実際には通学方法という項目もありますがここでは省略してます）．"
      ],
      "metadata": {
        "id": "iJRhh32NMHVj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zII3SHHlAOgc"
      },
      "source": [
        "このようなデータが与えられた場合，「小テスト」の点数を $x_1$，「睡眠時間」を $x_2$，「期末テスト」の点数を $y$ とおいて，\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2\n",
        "$$\n",
        "\n",
        "という式を考え，データに当てはまるパラメータ $w_0, w_1, w_2$ を決めることができれば，「小テスト」，「睡眠時間」の値と「期末テスト」の値の間にどんな関係があるのかを知ることができます．\n",
        "重回帰分析は，このようなデータ分析を行うための手法です．「小テスト」と「睡眠時間」が説明変数で，「期末テスト」が被説明変数です．この例では説明変数の数は2つですが，3つ以上の説明変数を考えることも可能です．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8mAMDUpc1NO"
      },
      "source": [
        "#### 実際に重回帰分析してみよう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGb4h3Y7V4qG"
      },
      "source": [
        "上記データの重回帰分析を実際にやってみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCTt4Igyauqw"
      },
      "source": [
        "# データの準備\n",
        "X = dfScore.drop(columns='期末テスト').values # 説明変数\n",
        "Y = dfScore['期末テスト'].values  # 被説明変数\n",
        "\n",
        "# 最小二乗法による平面のあてはめ\n",
        "lr = LinearRegression().fit(X, Y)\n",
        "w0 = lr.intercept_\n",
        "w  = lr.coef_\n",
        "\n",
        "# 得られたパラメータを表示\n",
        "print(f'w0 = {w0:.2f},   w1 = {w[0]:.2f},   w2 = {w[1]:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U0007mbYBIm"
      },
      "source": [
        "上記の結果は，「小テスト」と「睡眠時間」から「期末テスト」の値を予測する式が\n",
        "\n",
        "$$\n",
        "y = -1.87 + 6.43x_1 + 4.19x_2\n",
        "$$\n",
        "\n",
        "と求まることを意味しています．小テストの点数 ($x_1$) が高いほど，睡眠時間 ($x_2$) が長いほど，期末テストの点数 ($y$) は高くなるようです．\n",
        "これを使って実際にデータから「期末テスト」の点数を予測してみると..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzsWYgv5Y4Ei"
      },
      "source": [
        "# 予測値の計算\n",
        "Y_est = lr.predict(X)\n",
        "for n in range(len(X)):\n",
        "    print(f'{n:2d} 説明変数の値: {X[n]}  被説明変数の予測値: {Y_est[n]:.1f}  真の値: {Y[n]:d}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmPZtp8eZgIF"
      },
      "source": [
        "これだけでは予測がうまく言ってるかどうか判断が難しい（注）ですが，それっぽい値を出しているようです\n",
        "\n",
        "<font size=\"-1\">注） 単回帰の場合に **決定係数** を考えたように，重回帰分析でも当てはまりの良さや結果の信頼性を評価する方法があります．</font>\n",
        "\n",
        "単回帰分析の場合，説明変数を $x$，被説明変数を $y$ として，$(x,y)$ 平面にデータに当てはまる直線を引くことになっていましたが，上記の重回帰分析の場合，$(x_1, x_2, y)$ 空間に平面を当てはめることになっています．図に描くとこんなんなります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpT3BwVmUOTz"
      },
      "source": [
        "# 平面当てはめの可視化\n",
        "x = np.linspace(0, 10, num=11)\n",
        "y = np.linspace(0, 10, num=11)\n",
        "xx, yy = np.meshgrid(x, y)\n",
        "zz = w0 + w[0] * xx + w[1] * yy\n",
        "\n",
        "elevation, azimuth = 40, -30\n",
        "\n",
        "fig = plt.figure(facecolor='white', figsize=(12, 9))\n",
        "ax = fig.add_subplot(111, projection='3d', xlabel='$x_2$', ylabel='$x_1$', zlabel='$y$')\n",
        "ax.scatter(X[:, 0], X[:, 1], Y)\n",
        "ax.plot_wireframe(xx, yy, zz, color='red')\n",
        "ax.set_xlim(0, 10)\n",
        "ax.set_ylim(0, 10)\n",
        "ax.view_init(elevation, azimuth)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この図の赤線で描かれた平面は\n",
        "\n",
        "$$\n",
        "y = -1.87 + 6.43x_1 + 4.19x_2\n",
        "$$\n",
        "\n",
        "を表しています．"
      ],
      "metadata": {
        "id": "GCjJ97f_k9jJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Zorls_CB3V"
      },
      "source": [
        "#### 重回帰分析の定式化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4cpjAglJvkF"
      },
      "source": [
        "説明変数の数を $D$，データの数（サンプルサイズ）を $N$とすると，重回帰分析で扱うデータは次のように表せます．\n",
        "\n",
        "$$\n",
        "(x_{n, 1}, x_{n, 2}, \\dots, x_{n, D}, y_n)\\quad (n = 1, 2, \\dots , N)\n",
        "$$\n",
        "\n",
        "$n$ はデータの番号，$x_{n, 1}$ から $x_{n, D}$ までが $n$ 番目のデータの説明変数の値，$y_n$ が被説明変数の値を表わします． 上記の例では $D=2$ および $N=20$ でした．\n",
        "\n",
        "このデータに当てはめる式は，次のようなものです．\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2 + \\dots + w_Dx_D\n",
        "$$\n",
        "\n",
        "単回帰の式 $y = ax+b$ が2次元平面上の直線の式だったのに対して，この式は，データが広がる $(D+1)$ 次元空間中の平面（$D$次元超平面）を表わします．\n",
        "平面の位置や傾きを決めるパラメータは $w_0$ から $w_D$ までの $(D+1)$ 個あります．\n",
        "これらのパラメータは，単回帰の場合と同様に，最小二乗法によって求めることができます（注）．\n",
        "\n",
        "<font size=\"-1\">注） 単回帰の場合よりも複雑にはなりますが，微積分と線形代数を駆使してパラメータの連立方程式である「正規方程式」を導出することができます．\n",
        "その辺のことは「多変量解析及び演習」で登場予定です．\n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywUB7T94fUbs"
      },
      "source": [
        "----\n",
        "### 重回帰分析のさらに先へ\n",
        "\n",
        "重回帰分析のさらに先にも，いろいろ面白いものがあります．\n",
        "「機械学習」で学ぶことになるだろう内容ですが，一部簡単に紹介します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBzuUWZwgY15"
      },
      "source": [
        "#### 多項式の当てはめ and more\n",
        "\n",
        "重回帰分析の式をちょっと変形すると，単回帰で扱ったデータ\n",
        "$$\n",
        "(x_n, y_n)\\quad (n = 1, 2, \\dots , N)\n",
        "$$\n",
        "に対して，直線ではなく $D$ 次多項式\n",
        "\n",
        "$$ y = w_0 + w_1x + w_2x^2 + \\dots +\n",
        "w_Dx^D\n",
        "$$\n",
        "\n",
        "を当てはめることができます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOnZix3kENv"
      },
      "source": [
        "「ゴリゴリくん」でやってみましょう．以下のスライダーを動かして多項式の次数 $D$ を 1 から大きくしていってみよう．\n",
        "$D$ を大きくしていくと，大変なことになります...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqUbefwMkOnR"
      },
      "source": [
        "#@title あてはめる多項式の次数 {run: \"auto\"}\n",
        "D = 5 #@param {type:\"slider\", min:1, max:20}\n",
        "\n",
        "# ゴリゴリくん\n",
        "X, Y = dfGori['気温'], dfGori['アイス売上数']\n",
        "N = Y.shape[0]\n",
        "\n",
        "# データの行列 XX をつくる\n",
        "XX = np.empty((N, D+1))\n",
        "XX[:, 0] = 1\n",
        "for d in range(1, D+1):\n",
        "    XX[:, d] = X**d\n",
        "\n",
        "# 最小二乗法で多項式を求める\n",
        "lr = LinearRegression().fit(XX, Y)\n",
        "f_est = np.poly1d(np.hstack((lr.intercept_, lr.coef_[1:]))[::-1])\n",
        "\n",
        "# グラフを描く\n",
        "xr = np.linspace(0, 40, 100)\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(8, 6))\n",
        "ax.set_xlabel('Temparature')\n",
        "ax.set_ylabel('number of GoriGori')\n",
        "ax.scatter(X, Y, color=\"red\")\n",
        "ax.plot(xr, f_est(xr), '-', label=f\"D = {D}\")\n",
        "ax.set_xlim(0, 40)\n",
        "ax.set_ylim(-5, 125)\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# 得られた多項式を表示\n",
        "print(f_est)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuFToOnhomd-"
      },
      "source": [
        "この問題のように，与えられたデータに複雑な関数を当てはめる問題（**関数近似** と言うことがあります）は，科学技術の様々な問題に現れる重要なものです．\n",
        "関数近似問題への取り組み方（アプローチ）にも様々なものがありますので，数理・情報科学課程のあちこちの授業で出会うことになるかもしれません（微積分で学んだ「テイラー展開」も，その特殊な例と言えます）．\n",
        "いわゆる人工知能の基礎となる話でもありますので，「機械学習I/II」でも取り上げます．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig3dnpNMWPdh"
      },
      "source": [
        "#### ロジスティック回帰\n",
        "\n",
        "これまでの回帰分析の話では，被説明変数はすべて量的なものでした（注）．\n",
        "**ロジスティック回帰** は，被説明変数が質的なものでかつ2種類の値しかとらないようなデータを分析するための手法です．\n",
        "\n",
        "<font size=\"-1\">注）  説明変数の方については，質的な変数が混じっていたとしても，ダミー変数に変換してやることで，量的な変数とみなして普通に回帰分析することができます．</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qknwxXBvX3MM"
      },
      "source": [
        "例えばこんなん...．ある模擬試験を受験したのちある大学の入試を受験したひと300人分の，模試の2科目の点数と，大学入試に合格したか否か（1なら合格），のデータです．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfJozm3iXqnE"
      },
      "source": [
        "# 模試入試データ\n",
        "dfExam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzbQHZr5X_ij"
      },
      "source": [
        "模試の点数から合否を予測する式を作ることができれば，このデータに含まれない受験生の合否を予測できるかもしれません．このような問題を扱うのが，ロジスティック回帰です．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sndzEd4UZKHO"
      },
      "source": [
        "とりあえず，説明変数（模試2科目の点数）の散布図を描いてみるとこんなんなります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHPJdE7LZUs8"
      },
      "source": [
        "# データ\n",
        "X = dfExam[['模試数学', '模試英語']].values # 説明変数\n",
        "Y = dfExam['入試合否'].values # 被説明変数\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, facecolor='white', figsize=(12, 6))\n",
        "# 普通に散布図\n",
        "ax[0].scatter(X[:, 0], X[:, 1])\n",
        "ax[0].set_xlim(0, 100)\n",
        "ax[0].set_ylim(0, 100)\n",
        "ax[0].set_xlabel('Math')\n",
        "ax[0].set_ylabel('English')\n",
        "# 入試合否で色分け\n",
        "X0 = X[Y == 0, :]\n",
        "X1 = X[Y == 1, :]\n",
        "ax[1].scatter(X0[:, 0], X0[:, 1])\n",
        "ax[1].scatter(X1[:, 0], X1[:, 1])\n",
        "ax[1].set_xlim(0, 100)\n",
        "ax[1].set_ylim(0, 100)\n",
        "ax[1].set_xlabel('Math')\n",
        "ax[1].set_ylabel('English')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mA-yDMuashv"
      },
      "source": [
        "右の方は，合格したひとをオレンジで，不合格だったひとを青で表しています．さらに，入試合否も使って3次元の散布図を描くと，こんなことになります．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aakd_kgcbQV2"
      },
      "source": [
        "# 模試の点数と入試合否の3次元の散布図\n",
        "fig = plt.figure(facecolor='white', figsize=(10, 7.5))\n",
        "ax = fig.add_subplot(111, projection='3d', xlabel='Math', ylabel='English', zlabel='result')\n",
        "X0 = X[Y == 0, :]\n",
        "X1 = X[Y == 1, :]\n",
        "ax.scatter(X0[:, 0], X0[:, 1], Y[Y == 0])\n",
        "ax.scatter(X1[:, 0], X1[:, 1], Y[Y == 1])\n",
        "ax.set_xlim(0, 100)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_zlim(0, 1)\n",
        "elevation = 70\n",
        "azimuth = 240\n",
        "ax.view_init(elevation, azimuth)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS37lBZufMva"
      },
      "source": [
        "以下，ロジスティック回帰の式などの説明は省略して，結果を図示するだけにします．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2adi5xUXfh2z"
      },
      "source": [
        "# データ\n",
        "X = dfExam[['模試数学', '模試英語']].values # 説明変数\n",
        "Y = dfExam['入試合否'].values # 被説明変数\n",
        "\n",
        "# ロジスティック回帰\n",
        "lr = LogisticRegression(C=1e6).fit(X, Y)\n",
        "w0 = lr.intercept_[0]\n",
        "w = lr.coef_[0]\n",
        "\n",
        "# パラメータを表示\n",
        "print(w0, w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk2VE1urggsj"
      },
      "source": [
        "# 予測値\n",
        "x0 = np.arange(0, 100, 5)\n",
        "x1 = np.arange(0, 100, 5)\n",
        "xx0, xx1 = np.meshgrid(x0, x1)\n",
        "XX = np.vstack((xx0.reshape(-1), xx1.reshape(-1))).T\n",
        "yy = lr.predict_proba(XX)[:, 1].reshape(xx0.shape)\n",
        "\n",
        "# 可視化\n",
        "fig = plt.figure(facecolor='white', figsize=(10, 7.5))\n",
        "ax = fig.add_subplot(111, projection='3d', xlabel='Math', ylabel='English', zlabel='result')\n",
        "X0 = X[Y == 0, :]\n",
        "X1 = X[Y == 1, :]\n",
        "ax.scatter(X0[:, 0], X0[:, 1], Y[Y == 0])\n",
        "ax.scatter(X1[:, 0], X1[:, 1], Y[Y == 1])\n",
        "ax.plot_wireframe(xx0, xx1, yy, color='green')\n",
        "ax.set_xlim(0, 100)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_zlim(0, 1)\n",
        "elevation = 70\n",
        "azimuth = 240\n",
        "ax.view_init(elevation, azimuth)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ausDmjZBjcMG"
      },
      "source": [
        "上記の緑色は，ロジスティック回帰によって得られた予測式が表す曲面です．この式によって得られる予測値は，「合格する確率」と解釈することができます．\n",
        "\n",
        "以下は，予測値が 0.5 に等しくなるような説明変数の値の集合（緑色の直線）を描いたものです．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ4yagRwgquO"
      },
      "source": [
        "fig, ax = plt.subplots(1, facecolor='white', figsize=(6, 6))\n",
        "X0 = X[Y == 0, :]\n",
        "X1 = X[Y == 1, :]\n",
        "ax.scatter(X0[:, 0], X0[:, 1])\n",
        "ax.scatter(X1[:, 0], X1[:, 1])\n",
        "xr = np.linspace(0, 100)\n",
        "ax.plot(xr, -(w[0]*xr + w0)/w[1], color='green')\n",
        "ax.set_xlim(0, 100)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.set_xlabel('Math')\n",
        "ax.set_ylabel('English')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Ue4Vz-ksle"
      },
      "source": [
        "大雑把な考察ですが，この直線の傾きから，模試英語の点数よりも模試数学の点数の大小の方が入試合否に強く影響していることがうかがえます．\n",
        "\n",
        "ここでは説明しませんが，このロジスティック回帰を発展させて，ずっとずっと複雑な仕組みを考えると，文字認識，音声認識，画像認識といった問題も扱えるようになります．\n",
        "そういう話は，「機械学習I/II」の授業に出てくるでしょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA8twjzrrQOm"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}